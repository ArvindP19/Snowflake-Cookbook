--start by creating a new database which we will use to store results of our processing from Spark. To do so run the following SQL through Snowflake WebUI or SnowSQL. 
USE ROLE SYSADMIN;
CREATE DATABASE new_db;

--Paste the following code in snowflake_transform.scala file. Please make sure that you replace the placeholders in the code below with the information pertaining to your account.
val SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"

// initialise the connectivity related variables
var snowflakeOptions = Map(
    "sfURL" -> "<replace_with_your_account_url>",
    "sfUser" -> "<replace_with_a_user_name>",
    "sfPassword" -> "<replace_with_password>",
    "sfDatabase" -> "NEW_DB",
    "sfSchema" -> "PUBLIC",
    "sfWarehouse" -> "<replace_with_the_virtual_warehouse>"
)

// validate that connectivity works by querying a sample table
//read and output a table
spark.read
    .format(SNOWFLAKE_SOURCE_NAME)
    .options(snowflakeOptions)
    .option("dbtable", "REGION")
    .load().show()

--Now run spark-shell and run the following command.
:load snowflake_transform.scala

--Let us now run a query through Spark. The query will aggregate sales data from the sample database. To do so append the following code snippet to your scala file.
val aggDFReader: DataFrameReader = spark.read
    .format("net.snowflake.spark.snowflake")
    .options(snowflakeOptions)
    .option("query", """SELECT C.C_MKTSEGMENT AS MARKET_SEGMENT, SUM(O_TOTALPRICE) AS REVENUE 
						FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS O 
						LEFT OUTER JOIN SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER C 
						ON O.O_CUSTKEY = C.C_CUSTKEY 
						GROUP BY C.C_MKTSEGMENT;""")   

val aggDF: DataFrame = aggDFReader.load()	
aggDF.show()

--run spark-shell and run the following command.
:load test_sf_connect.scala

--Now that we have our aggregate results, we are going to write them to a new table. To do so append the following code snippet to your scala file.
aggDF.write
    .format("snowflake")
    .options(snowflakeOptions)
    .option("dbtable", "CUST_REV")
    .mode(SaveMode.Overwrite)
    .save()

--Now run spark-shell and run the following command.
:load test_sf_connect.scala

--via Snowflake WebUI or SnowSQL run the following command to validate that the table was successfully loaded.
SELECT * FROM new_db.PUBLIC.CUST_REV;